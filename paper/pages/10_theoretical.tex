% Theoretical Framework
\subsection{Theoretical Framework}

This research attempts to solve an issue facing most Visual Sensory Substitution Devices which is primarily about the issue on translating a high bandwidth visual information to low bandwidth audio information without causing overstimulation, our novel solution is the emulate the human sensory mechanism to carry the processing load from the user and ensure that substituted information is minimal and nessary to the user. The theoretical principles includes:

\begin{enumerate}
    \item \textbf{Focus and Spatial Resolution (Foveation)} - The human vision has a tiny high-resolution fovea covering around 1 to 2 degrees of the visual field but accounts for around half of visual cortex \parencite{krantz_2012}, outside this region is a much coarser peripheral vision where acuity drops rapidly \parencite{iwasaki_1986}. \\ Our device attempts to mimic this by using a focus point system where the user can contain the focus radius indicanting that area of the environment the user wants to be translated into audio and at what detail.
    \item \textbf{Selective Attention and Daliency} - Has the visual system cannot process all details at once, it selectively attends to salient or task-relavant features, this is done by implementing a bottom-up saliency and top-down goals to filter out redundant/irrelevant information \parencite{kristjansson_2016}. This is to mean that people focus on key objects ignoring uniform backgrounds as the nervous system "tunes out" repeated stimuli and amplifies novel/focused ones \parencite{gershman_2024}. \\ Has the device translates environmental information, any stagnating/constant information must be tuned out over time leaving behind changes indicating motion.
    \item \textbf{Scene Gist and Gestalt Organization} - Human vision rapidly extract the gist (background) and figures (foreground) from a scene within the first fixation (~36ms) with around 80\% accuracy \parencite{loschky_2025}. This process is done through Gestalt principles that groups elements to simplify complex images such as by similarity, proximity, common region, continuity, etc. \parencite{usertesting_2024} \\ This indicates that the device should be able to generally/primitively seperate the environmental data into background and foreground categories where foreground can then be seperated into groups, this is information to indicate the translated audio.
    \item \textbf{Parallel Motion vs. Detail Pathways} - Human visual systems process motion and detials in two seperate channels, the \textbf{magnocellular pathway} for fast motion and size but in less detail, and the \textbf{parvocellular pathway} for static object in more detail \parencite{zeki_2015}. \\ This suggest that when our device switches to motion detection it should prioritize the speed, size, and direction of that motion.
    \item \textbf{Temporal Dynamics and Scanning} - Vision is not a singlular static snapshot but rather continuous sample of the world via eye movements has humans typically shift gaze 3-4 times a second \parencite{kristjansson_2016}. \\ Thus our device should rapidly provide updating audio to allow temporal integration rather than one large static soundscape all at once, this also solves the issue of cognitive overload and sensory fautigue.
    \item \textbf{Multisensory Integration} - The brain integrates additional senses like auditory, vestibular, and proprioceptive senses to form a coherent representation of the environment \parencite{kristjansson_2016}. \\ Thus our device should align with those senses to prevent conflicting senses that could often cause nausea. This could be in a form of aligning the virtual environment with IMU to align with vestibular senses, or lower the output audio when the user is focusing on something else like talking to others.
\end{enumerate}
