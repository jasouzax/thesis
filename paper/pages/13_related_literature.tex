\subsection{Review of Related Literature}

\textcite{zvori_teanu_2021} presented the \textbf{Sound of Vision (SoV) system}. It is a wearable SSD designed to aid VIPs in spatial cognition and navigation, using stereo-vision-based 3D reconstruction to interpret the environment and translate spatial data into audio and haptic cues. The device integrates a stereo camera, an infrared depth sensor, and an inertial measurement unit (IMU) to track the motion of the user’s head and the installed camera mounted on the headgear, using these as its inputs. Furthermore, the output audio and haptic feedback are delivered via headphones and a haptic belt. The device processes visual data in real time using GPU-accelerated computation to reconstruct the 3D Environment and identify obstacles via segmentation algorithms based on disparity and histogram analysis. The depth of each pixel is determined by the stereo disparity formula $Z=\frac{f\cdot B}{d}$, where $Z$ is the depth, $B$ is the baseline, $f$ is the focal length, and $d$ is the disparity. The SoV converts environmental data into audio-haptic cues, allowing users to detect obstacles, open spaces, and open areas. In testing the device's performance with the visually impaired participants, it showed a $10\%$ improvement in depth accuracy and an $88.5\%$ task success rate compared to standard stereo vision methods. The results highlight the device's effectiveness in real-world navigation and its potential to enhance mobility and spatial awareness compared to traditional aids such as white canes.

\textcite{sami_2025} developed a \textbf{smart wearable (hat) assistive device} that integrates object detection with voice-assisted navigation to support VIP in real-time spatial awareness. The device uses an ESP32-CAM module to capture live video and transmit it via Wi-FI to a mobile application that employs a TensorFlow-based deep learning model for efficient object recognition. The camera-detected objects are then processed into audio prompts that send alerts to the user when there are nearby obstacles and provide navigation directions. The programmed system produced a seamless object-detection-to-voice pipeline. The Smart Hat's system architecture demonstrates the integration of an IoT processing method to enable wireless communication between the smart hat and the user interface for remote processing and real-time updates for the user. Testing results show an object detection accuracy of over $91\%$, highlighting the reliability in dynamic environments. In addition, the Smart Hat device offers compact, low-cost, and user-friendly features. It demonstrates how IoT-enabled computer vision can transform traditional assistive technologies into intelligent, voice-guided navigation tools for the visually impaired.

\textcite{realsense2025}, in collaboration with Eyesynth, developed a wearable sensory-substitution device designed to enhance spatial awareness and independent navigation for visually impaired users by converting real-time 3D spatial data into bone-conduction audio cues. The device is called \textbf{Non-Invasive Image Resynthesis into Audio (NIIRA)}. The device employs the RealSense D415 depth camera to capture real-time 3D depth data calculated using the stereo disparity formula $Z=\frac{f\cdot B}{d}$. Then it is processed to generate a point cloud representation of the surroundings, where each depth pixel being mapped to 3D coordinates using the equation $X=(u-c_x)\frac{z}{f_x},\;Y=(v-Cy)\frac{Z}{f_y},\;Z=Z$ to provide a complete spatial surrounding rather than a 2D view. The point-cloud data conjoint with the inertial and head motion tracking, is then processed through an onboard ASIC and Simultaneous Localization and Mapping (SLAM) modules to identify object orientation, translating them into audio signals such as pitch (height), volume (distance), and stereo panning (lateral position), allowing users to perceive objects and obstacles up to 5 meters away without external infrastructure. In testing NIIRA, it demonstrated improved independence and navigation in both indoor and outdoor environments, adapting to users’ preferences, and displayed high obstacle-detection accuracy, minimal processing latency, and enhanced independent navigation. The participants reported that the bone-conduction audio feedback provided a clear environmental awareness without blocking external sounds.

\textcite{udayakumar_2025} designed a \textbf{Smart Vision Glasses (SVG)} that processes environmental input data using AI, LiDAR depth mapping, and computer vision algorithms. The device’s front-facing camera captures visual data, while the LiDAR sensor measures real-time depth information using the principle of Time-of-Flight (ToF) – where the distance (D) is calculated using the formula $D=\frac{c\cdot t}{2}$, where $c$ represents the speed of light and $t$ the time taken for emitted light to return after reflection. These depth and image data are then processed through an AI-based recognition model that uses Convolutional Neural Networks (CNNs) to classify objects, text, and faces within the user's field of view. The recognized elements are subsequently analyzed and prioritized based on proximity and relevance using the LiDAR-assisted spatial mapping, which produces a semantic understanding of the scene; implementing Volume of Interest (VOI), a controllable spatial region or focus radius that limits feedback, to prevent sensory overload, enhancing user focus in dynamic environment. This processed information is then transmitted to a smartphone-based application converting the recognized data into voice audio cues through text-to-speech (TTS) engine. The device applies natural language processing (NLP) principles to ensure that the audio output is contextually meaningful and user-friendly. The input-output process follows a sequential pipeline:
\begin{enumerate}
    \item capture of the image depth data
    \item AI-driven detection and classification
    \item distance estimation through the ToF equation
    \item conversion of results into voice audio output describing nearby objects, text, or faces
\end{enumerate}
The SVG through hearing supports the four primary modes – “Thighs Around You”, “Reading”, (Walking Assistance), and “Face Recognition”. The SVG is also equipped with gesture and voice control interfaces, allowing users to switch modes or issue commands hands-free, enhancing accessibility and user interaction. A multicenter usability study was conducted across five rehabilitation centers in India, involving 90 participants with a mean age of 23.5 years, to test the device functionality. The participants tested the four primary modes by completing real-world navigation and recognition tasks, and their feedback on the device's usability and helpfulness was also recorded. Results showed that $72.9\%$ indicated that "Reading” mode is helpful, $44.7\%$ for “The Things Around You”, $36.5\%$ for “Face Recognition”, and $22.4\%$ for “Walking Assistance”, showing that the device effectively enhances environmental awareness, reading ability, and object identification.

\textcite{ruan_2025} developed a multifaceted sensory substitution wearable device that uses an audio-based curb detection to improve real-time awareness and navigation safety for individuals who are blind or have low vision. The device integrates a stereo camera, ultrasonic range sensors, and inertial motion units (IMUs) to identify curbs and ground-level transitions Adapting the stereo disparity formula $Z=\frac{f\cdot B}{d}$, the system calculates the curb height and distance, while the ultrasonic sensors confirm the surface proximity of the environment for redundancy and accuracy. The sensory data are processed and converted into audio and vibrational cues, where the pitch and repetition rate of the sound correspond to the curb’s distance and height, providing intuitive, time-sensitive feedback. In the pilot testing, 12 participants were involved, 8 with blindness and 4 with low vision. The participants tested the navigation feature of the device in both indoor and outdoor test environments, including simulated sidewalks, ramps, and descending curbs. After testing, the performance statistics of the device showed an average curb detection accuracy of $94.3\%$, with a false-negative rate of $3.7\%$, and an average alert response time of 1.2 seconds, indicating the system’s reliability in detecting and signaling curbs in real time. Participants reported that it also improved confidence in mobility and reduced the risk of missteps or trips. This demonstrates that the audio-based curb detection approach driven by LiDAR-like depth estimation and multimodal feedback effectively enhances real-time alerting and safe navigation for the VIP.

\textcite{viancy_2024} introduced AuralVision, a wearable assistive device that is designed as eyewear to improve navigation for visually impaired individuals by incorporating object detection, scene classification, and reinforcement learning. The device's system architecture includes a camera sensor to capture the user’s environment, image-processing and object-detection software to classify obstacles and identify significant objects, and an audio module to translate detection results into auditory cues for navigation. Utilizing a convolution neural network (CNN) approach or deep learning-based vision algorithms trained on the Object Net 3D dataset to interpret the surrounding environment and convert the visual information to auditory cues that convey spatial awareness. AuralVision uses 3D visualization and sound-based feedback to identify objects, classify road scenes, and assist users with obstacle avoidance and pathfinding. The device adapts to dynamic environments through continuous learning, thereby improving navigation decision-making.

\textcite{hamilton_fletcher_2021} developed a mobile sensory-substitution device called SoundSight. This device translates input color, depth, and temperature data into output audio cues, allowing the visually impaired users to sense their environment. The system captures environmental input using a mobile with an RGB-D camera and a thermal sensor. Then it processes the data stream through a multi-feature mapping pipeline before integrating it into a composite audio stream. Depth is calculated using the stereo disparity formula to determine the distance of the object from the user's perspective. At the same time, the RGB camera's color hue values are mapped to sound pitch using the equation $f_c=k_c\times H$, where $H$ represents the hue intensity and scaling constant. Temperature measurements are converted to amplitude modulation through $A_t=k_t\times T$, where $T$ is the sensed temperature and $k_t$ is a sensitivity coefficient. These parallel mappings are integrated using the data fusion algorithms, synchronizing depth, color, and thermal inputs before generating the composite audio streams that encode environmental depth through rhythm, color through pitch, and temperature through loudness. In testing the device's functionality, 15 blind and 10 low vision participants are involved. The device demonstrated rapid user learning and over $85\%$ recognition accuracy, demonstrating that the multi-feature sonification can reliably convey complex environmental information.

\textcite{han_2026} designed a Multi-Path Sensory Substitution Device (MSSD) that enhances low-vision mobility and virtual navigation of the visually impaired people by integrating real-time depth mapping, IMU-based motion tracking, and depth optimization algorithms. The system processes input from a depth camera and inertial sensors to create a 3D spatial model of the environment, using the stereo disparity equation to calculate object distance and the projection model $X=(u-c_x)\frac{z}{f_x},\;Y=(v-c_y)\frac{z}{f_y}$ to map each pixel to world spatial coordinates. After acquiring the data from the processed input, it generates a 3D point cloud, which then is converted into an occupancy grid for spatial awareness, while the IMU readings refine user orientation using the equation $\theta_t=\theta_{t-1}+\omega t\Delta t$. A modified A* (A-star) algorithm is used to compute the optimal navigation routes using the cost function $f(n)=g(n)+h(n)$, balancing actual distance and heuristic estimates to avoid obstacles. The selected virtual path is then translated into audio-haptic feedback, where the vibration intensity and sound frequency indicate direction and proximity, using speakers or headphones to create output data directional audio cues and vibration motors/haptic actuators for tactile feedback. To check the device's functionality, the researchers tested it with 20 low-vision participants. The system attained a result of $91\%$ path following accuracy and a $38\%$ reduction in navigation errors. As the device demonstrates a satisfactory functional performance, this indicates that combining 3D mapping, motion sensing, and heuristic path planning effectively supports real-time, safe, and virtual navigation for the visually impaired individuals.

\textcite{commere_2023} evaluated five depth-to-sound sonification methods to compare their performance and effectiveness. One of the methods evaluated is the LiDAR-to-repetition-rate sonification model. The identified sonification model converts visual information into rhythmic auditory cues, helping visually impaired individuals to develop a sense of spatial awareness through sound. Using the Time-of-Flight (ToF) equation $D=\frac{c\cdot t}{2}$, the model measures the distance of an object and maps it inversely to the beep repetition rate $R=\frac{k}{D}$, where $R$ is the repetition rate (Hz), $D$ is the distance (meters), and $k$ is a scaling constant which is determined experimentally to maintain the perceptible tempo differences between near and far objects which results if an object is closer it generates faster repetition rates or beeps and far detected objects have slower rhythm beeps which the model’s spectrogram design reflects this relationship. Whereas the temporal density (number of pulses per second) indicates proximity, while frequency and amplitude encode additional object features such as height and reflection intensity, creating a dynamic “rhythmic depth map” that represents distance through time spacing between sound bursts—in experimental testing with 28 sighted but blindfolded participants, a three-phase protocol consisting of Depth Estimation, Azimuth Estimation, and Retention Assessment confirmed that repetition-rate sonification produces the lowest mean absolute error (MAE) in distance perception and was the most intuitive and accurate among the other mapping methods like frequency or amplitude-based cues. The results finding revealed a strong inverse correlation between the perceived distance and repetition rate ($R\propto\frac{1}{D}$), indicating that LiDAR-driven rhythmic approach effectively translates the spatial depth into comprehensive sound patterns, offering an enhanced basis for real-time auditory sensory substitution systems for the visually impaired.

\textcite{zhao_2025} conducted a clinical evaluation of an assistive device, a wearable electronic navigation aid (ENA), for visually impaired individuals through a prospective, non-randomized, single-arm, open-label trial involving 30 participants (each participant was given five trials for each functional task), selected through purposive sampling to ensure that individuals with blindness or severe visual impairment could assess the performance of the assistive device. The study primarily focused on the functional efficacy of the device, assessing how effectively it supports navigation and daily tasks of the visually impaired users. The assistive device used in testing integrates ultrasonic sensors, vibration motors, and audio output components. The study implemented a structured, protocol-guided or functional evaluation framework that combined quantitative measures including accuracy rate ($\frac{Sucessful\;Trails}{Total\;Trails}\times 100$), error rate ($\frac{Navigation\;Errors}{Total\;Trails}\times 100$), reaction time ($RT=T_{response}-T_{stimulus}$), task completion time ($TCT=T_{end}-T_{start}$), and performance improvement rate ($PIR=\frac{(Post\;test-Pre\;test)}{Pre\;test}\times 100$) to assess navigation accuracy, responsiveness, and efficiency objectively, supported by qualitative user feedback on comfort, usability, and confidence. The study reveals that the ENA device effectively improved safe navigation and mobility performance for visually impaired users.

\subsection {Summary of Related Literature}

Recent studies (2019–2025) on assistive technologies for the VIP highlight wearable and mobile devices utilizing a sensory substitution approach to convert visual/spatial data into audio, haptic, or multimodal cues, applying stereo vision (e.g., disparity formula $Z=\frac{f\cdot B}{d}$), depth sensors like LiDAR/ToF (e.g., $D=\frac{c\cdot t}{2}$), AI-driven computer vision (e.g., CNNs, TensorFlow), and IMUs. This approach expresses the outcome produced by the device such as real-time obstacle detection, object recognition, and navigation, similar to the Caraiman et al.'s Sound of Vision (SoV), which addresses sensory overloading via a controllable focus radius (VOI) and modular components for partial generalizability; Sami et al.'s Smart Hat for cost-effective voice output; RealSense's NIIRA point-cloud processing to semantic audio; Commère and Rouat's LiDAR sonification for rhythmic depth cues; Hamilton-Fletcher et al.'s SoundSight for mobile timbre/volume mappings; Ruan et al.'s curb detector for $94.3\%$ accuracy; Han and Li's Multi-Path Sensory Substitution Device (MSSD) for $91\%$ path accuracy; Zhao et al.'s electronic navigation aid (ENA); and Udayakumar and Gopalakrishnan's Smart Vision Glasses (SVG) for AI-LiDAR voice modes. However, systems like AuralVision that utilizes ToF lasers for bone-conduction stereo sound, lab-effective but range-limited, Eyesynth’s NIIRA object recognition process prone to overload without human emulation, Depth Sonification repetition rates for intuitiveness, SoundSight LiDAR/thermal to audio hwich is app-constrained, Smart Hat that is non-modular, and multi-faceted SSDs with $85\%$ curb accuracy often face issues like sensory overload, training demands, limited real-world adaptability, and lack of digital-physical navigation. These studies proposes an idea that work closely aligns with SoV by tackling sensory overloading through VOI and modular components but advances it by explicitly mimicking human perception via a modes system and a modular IoT pipeline that applications can embed for navigation in both physical and digital worlds, addressing SoV's training needs and real-world performance gaps while incorporating brain emulation for superior overload reduction and versatility, building on these predecessors to offer a more intuitive, scalable solution for VIP independence. In addition, evaluations across studies ranging from 12 to 90 participants showcase $85\%$ to $95\%$ accuracies and qualitative benefits like reduced errors (e.g., $38\%$ in MSSD). These studies introduce an innovative and improved SSD by adding brain emulation, AI, and IoT for advanced overload reduction and versatility. 
