% Theoretical Framework
\subsection{Theoretical Framework}

This research attempts to solve an issue facing most Visual Sensory Substitution Devices which is primarily about the issue on translating a high bandwidth visual information to low bandwidth audio information without causing overstimulation, our novel solution is the emulate the human sensory mechanism to carry the processing load from the user and ensure that substituted information is minimal and nessary to the user. The theoretical principles includes:

\begin{enumerate}
    \item \textbf{Focus and Spatial Resolution (Foveation)} - The human vision has a tiny high-resolution fovea covering around 1 to 2 degrees of the visual field but accounts for around half of visual cortex \parencite{krantz_2012}, outside this region is a much coarser peripheral vision where acuity drops rapidly \parencite{iwasaki_1986}. \\ Our device attempts to mimic this by using a focus point system where the user can contain the focus radius indicanting that area of the environment the user wants to be translated into audio and at what detail.
    \item \textbf{Selective Attention and Daliency} - Has the visual system cannot process all details at once, it selectively attends to salient or task-relavant features, this is done by implementing a bottom-up saliency and top-down goals to filter out redundant/irrelevant information \parencite{kristjansson_2016}. This is to mean that people focus on key objects ignoring uniform backgrounds as the nervous system "tunes out" repeated stimuli and amplifies novel/focused ones \parencite{gershman_2024}. \\ Has the device translates environmental information, any stagnating/constant information must be tuned out over time leaving behind changes indicating motion.
    \item \textbf{Scene Gist and Gestalt Organization} - Human vision rapidly extract the gist (background) and figures (foreground) from a scene within the first fixation (~36ms) with around 80\% accuracy \parencite{loschky_2025}. This process is done through Gestalt principles that groups elements to simplify complex images such as by similarity, proximity, common region, continuity, etc. \parencite{usertesting_2024} \\ This indicates that the device should be able to generally/primitively seperate the environmental data into background and foreground categories where foreground can then be seperated into groups, this is information to indicate the translated audio.
    \item \textbf{Parallel Motion vs. Detail Pathways} - Human visual systems process motion and detials in two seperate channels, the \textbf{magnocellular pathway} for fast motion and size but in less detail, and the \textbf{parvocellular pathway} for static object in more detail \parencite{zeki_2015}. \\ This suggest that when our device switches to motion detection it should prioritize the speed, size, and direction of that motion.
    \item \textbf{Temporal Dynamics and Scanning} - Vision is not a singlular static snapshot but rather continuous sample of the world via eye movements has humans typically shift gaze 3-4 times a second \parencite{kristjansson_2016}. \\ Thus our device should rapidly provide updating audio to allow temporal integration rather than one large static soundscape all at once, this also solves the issue of cognitive overload and sensory fautigue.
    \item \textbf{Multisensory Integration} - The brain integrates additional senses like auditory, vestibular, and proprioceptive senses to form a coherent representation of the environment \parencite{kristjansson_2016}. \\ Thus our device should align with those senses to prevent conflicting senses that could often cause nausea. This could be in a form of aligning the virtual environment with IMU to align with vestibular senses, or lower the output audio when the user is focusing on something else like talking to others.
\end{enumerate}

Aside from these principles our device should be able to assist the user by navigating the physical world and the digitial world. While the principle assists with navigating the physical world like environment, object, etc. we implement AIoT to assist the user navigating the digital world like google maps, weather, social media platforms, etc. This is where the device will act as the client while the AIoT is managed by the server, as seen in figure \ref{fig:architecture}.

\begin{figure}[H]
    \centering
    \caption{AIoT System Architecture}
    \includegraphics[width=\linewidth]{architecture.png}
    \label{fig:architecture}
\end{figure}

The figure \ref{fig:architecture} shows the following components of the system:
\begin{enumerate}
    \item \textbf{Sensors} - The sensors captures stereo images through camera, user's head orientation through IMU, and voice commands through Microphone.
    \item \textbf{Microcontroller} - The raspberry pi zero 2w is the main microcontroller that handles client side processes like the five provided applications.
    \item \textbf{Actuators} - The Speaker from the earphone is the only major actualor of the system.
    \item \textbf{Applications} - There are five major client side applications that interacts with the virtual maps such as to Emulate Human Vision, Photogrammetry, Hand Guesture Recognition, Voice Recognition, and Server API. Two of these application uses client side AI models:
        \begin{enumerate}
            \item \textbf{Hand Gesture Recognition} - \textbf{MediaPipe} an Open-source model by Google is used to detect the hand gesture from the user.
            \item \textbf{Voice Recognition} - \textbf{WhisperX} by OpenAI is used to recognize instructions/infomations provided by the user
        \end{enumerate}
    \item \textbf{Virtual Maps} - There are two kinds of map that stores and manages two kinds of data, data that is plotted into physical space called the Physical Map, and data is not plotted into physical space called Digital Map.
    \item \textbf{Server Side} - The server side is responsible for dealing for dealing with client data and interact with external APIs such as:
        \begin{enumerate}
            \item \textbf{Web APIs} - Interacts with web APIs such as google maps for directions or social media for communication
            \item \textbf{Local AIs} - Processes local AI such has Ollama GPT-OSS for assistant without actual humans or Yolo for advance image recognition/segmentation
        \end{enumerate}
\end{enumerate}

The system is as follows:
\begin{enumerate}
    \item The stereo camera with help from the IMU is processed by the Photogrammetry Application to generate the Physical Map, where the Emulated Vision Application can process it into audo cues.
    \item The right camera is individually processed by the Hand Gesture Application to detect if the guesture matches any instruction, if it does it records it in the Digital Map.
    \item If the Digital Map requests to enable voice commands then the Microphone is processed by the Voice Recognition to be provided to the Digital Map.
    \item If the Digital Map requests to access Network data such as GPS or Server APIs then that request is sent to the central Server.
    \item The central server processes the request either by running the AI models or making other external API request to fulfill the request, the response is sent back to the client's Digital Map.
    \item Once the Digital Map want to provide information to the user it is sent to the Physical Map which is then sent the the Emulated Human Application to generate the final Audio.
\end{enumerate}


%The connections of between each components allowing this to work could be seen in figure \ref{fig:blockdiagram}, such as the connections of each materials with relation to the raspberry pi.